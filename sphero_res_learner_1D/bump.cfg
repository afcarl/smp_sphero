# EH learner Sphero

# learner parameters
[learner]
# modes: ramp, bump, PIDvel, vel
mode = ramp
# reservoir time constant tau \in [0, 1[
# tau = 0.01
tau = 0.2
# tau = 0.9
# reservoir gain g \in [0, ...]
# g = 1.5
# g = 1.2
g = 0.999
# g = 0.7
# g = 0.

# motor-sensor delay
# lag = 0 
# lag = 1
# lag = 2
lag = 6

# exploration noise sigma, added to current network output
res_theta = 1e-1

# learning rate, well ...
# eta_EH = 1e-1
eta_EH = 1e-2
# eta_EH = 1e-3
# eta_EH = 1e-4

# a placeholder target
target = 0.4

# global input gain
res_input_scaling = 1.

# reservoir global feedback gain
res_feedback_scaling = .0
# reservoir global bias gain
res_bias_scaling = 0.1

# reservoir global output gain
res_output_scaling = 100.

# network input dimension
# sphero: 3 orientation, 3 ang vel, 3 linear acc, 2 pos, 2 vel
network_idim = 2
# network output dimension
network_odim = 1

# select input channels by index, don't think that's used in sphero exprs
input_coupling_mtx = {0: 1.}
# input_coupling_mtx = {0: 1., 1: 1., 2: 1., 3: 1.}

# reservoir nonlinearity
nonlin_func = np.tanh
# nonlin_func = lambda x: x

# reservoir network size
network_size = 100
# reservoir network connection density
p = 0.1
# reservoir neuron state noise, the larger the more L2 style weight regularization
network_theta_state = 1e-2

# time constant for differential hebb
coeff_a = 0.2

# stuff
use_ip = 0
use_pre = 0
pre_inputs = []
pre_delay = []
use_et = 1
et_winsize = 10
use_anneal = 1
anneal_const = 2500.
use_mt = 0
use_density = 0
density_mode = 0
use_wb = 0
wb_thr = 1.
do_savelogs = 0
pm_mass = 1.

# experiment parameters
[experiment]
# episode length
len_episode=2040
len_washout = 10
ratio_testing = 0.8
anneal_const = 200000.
tp_perturbation = {}
tp_target = {"ramp": 1}
target_interval = 5000
